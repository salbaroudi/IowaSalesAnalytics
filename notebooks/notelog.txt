#June 27th:

- Project has been loaded.
- installed dask[complete] via pip3.

- Today, I want to:
   - load the data frame with dask in a lazy manner.
   - be able to produce a subset of the data, with the
   following features:
      - pick out columns that we wish to keep
      - impose a limit on how many lines to read
      - randomly sample X lines from the file.
      - derive new columns before we write them
      to a file.


- there are triple quotes and EOF issues that
cause our parser to crash. I choose to skip
bad lines. If 1/5000 lines is lost, it won't matter
on a dataset with 12M rows. This means 2400 rows
will be lost.Y

#June 28th:
- What kind of work was done on this dataset, from others:
- data reading:
   - there is a 10% copy of the data, to use pandas directly.
   - BigQuery/SQL Alchemy in order to process the data.

- Analysis:
   - linear regression (factors that affect sales)
   - Summary Statistics
   - Geospatial data (sales)
   - joins with other data: Iowa population per town.


There are a few Significant Data Science Projects
that were done:

1) Jim Lung's Final Project:
    - Correlation of factors that affect sales price
    - Forward Regression of sales price
    - Log Tranformation on non-gaussian data (high
    range)

2) Baur Safi: Linear Regression on 10% of the dataset.


What is your goal for the analysis:
Exploratory Analysis:
1) Look at the columns, and describe with summary
statistics
2) Correlation between numerical columns (the corr)
method.
3) Unsupervised learning, and seeing if there are groups.
4) Basic Map of sales in the region, by county.

And pump out some graphs on the data pipeline, basically.

#June 30th:

I've gotten stuck on loading the full dataset. There
are a few issues: I seem to lose about 4/5ths of the data.
There are also issues with columns: 6 columns are just
ignored by Dask. Not sure whats going on.

I am going to take the 10% dataset, and start my analytics
and pipeline immediately.


Other things to look at:
- see Katie's Kaggle for log transform, and reasons to
use it.



#July 2nd:

- I figured out why columns were missing from the data frames.
- The 10percent dataset already had columns cut out. I mistakenly
thought that it had all 24 cols.
- I can easily read in the reduced dataset now. I will start with this
and produce analytics before I move to the full set.
- A lot of data tidying today. Dataset should be ready tomorrow, to make
basic Tableau visualizations.


#July 9th:

-after working .on the summary (yesterday), I realized that nobody really
deals with the full data frame for this dataset. I can add value by applying
Dask, and creating subset of the data.

- I need to first load the entire dataset, and calculate summary statistics.
- There are erroneous rows, duplicate rows as well (check for both)
- check that totals add up (for regression). Reject any rows that don't match.
- check for NAs.
- Get a count on total rows, and the losses we get as we clean up the dataset.
